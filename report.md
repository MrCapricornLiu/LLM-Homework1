# 1. Tokenization
# 1.1 实现BPE，训练Tokenizer（6分）
## 【1分】在实验报告中简要概述一下BPE算法和基于BPE算法训练LLM tokenizer的流程。
### BPE算法概述
BPE算法 (Byte Pair Encoding) 是一种基于统计的方法，最初用于文本压缩，但被广泛应用于自然语言处理（NLP）中作为词汇切分的工具。其核心思想是通过不断合并最频繁的字符对，逐步构建一个子词级别的词汇表，从而提高模型处理词语的灵活性和效率。

### BPE算法的基本步骤：  
- 初始化： 把输入的文本转化为字符级别的表示（即将单词分解为字符）。  
- 计算频率： 统计文本中所有相邻字符对（bigram）的频率。  
- 合并操作： 每次合并频率最高的字符对，形成一个新的“子词”单元，并替换原文中的该对字符。  
- 重复步骤： 重复合并步骤直到达到设定的词汇大小（vocab_size）。  

### 基于BPE算法训练LLM Tokenizer的流程：
- 文本预处理： 将训练数据进行预处理，例如分句、去除无用字符等，得到一个纯文本的字符串。  
- 初始化tokenizer： 创建一个空的tokenizer并初始化基本的词汇表。初始的词汇表通常包含所有单一字符和一个特殊的\<unk\>标记。  
- BPE训练： 使用BPE算法从训练数据中逐步合并字符对，更新词汇表直到达到目标词汇大小（vocab_size）。  
- 编码与解码： 使用训练好的tokenizer对新的文本进行编码和解码操作，将文本转化为模型可以处理的token ID序列，或将token ID序列转换回原始文本。  
## 【2分】实现一个基于BPE算法的tokenizer
见bpe/bpe_tokenizer.py内的class Tokenizer。

## 【1分】hw1-code/bpe/manual.txt是《北京大学研究生手册（2023版）》中提取出来的文字，请你用它来训练你的tokenizer，vocab_size为1024。

见bpe/main.py内的具体代码。


## 【1分】用它来encode再decode manual.txt，检查与原始manual.txt是否完全一致？

- 见bpe/main.py内的具体代码。
- 比较结果：完全一致。

## 【1分】学习使用huggingface transformers中的tokenizer，使用它加载GPT-2的tokenizer，然后使用它和你训练的tokenizer分别encode以下句子，比较两者的输出，简要解释长度上和具体token上不同的原因是什么。
- 比较结果（具体代码见bpe/main.py）
    - BPE Tokenizer for Sentence 1: ['O', 'r', 'i', 'g', 'i', 'n', 'a', 't', 'e', 'd'] Total tokens: 953
    - GPT-2 Tokenizer for Sentence 1: [11610, 3898, 355, 262, 11773, 2059, 286, 350, 18754, 287] Total tokens: 185
    - BPE Tokenizer for Sentence 2: ['博', '士', '学', '位', '论', '文', '应', '当', '表', '明'] Total tokens: 149
    - GPT-2 Tokenizer for Sentence 2: [39355, 248, 18803, 27764, 99, 19526, 235, 164, 106, 118] Total tokens: 306
- 差异及原因
    - 长度差异
        - GPT-2的tokenizer是基于子词（subword）单元的，它会使用大量的预定义词汇表中的子词单元来表示输入句子。由于GPT-2的tokenizer已经有了大量的子词单元，它可能将一些常见的词（如Peking University、China等）映射为一个token，导致token数量较少。
        - 训练后的BPE tokenizer在词汇表大小较小的情况下，可能会更多地使用字符级别的token进行编码，这会导致token数量更多，编码结果也会有所不同。
    - 具体Token差异
        - GPT-2的tokenizer会使用其大规模的预训练词汇表来表示常见的单词和词组，因此它会更多地使用已知的子词单元（例如Peking University可能会作为一个token表示）
        - 而BPE tokenizer可能会拆分为更小的单元。
        - 例如，BPE可能会将"Peking University"分为多个token（如Pek, ing, Univer, sity等），而GPT-2 tokenizer可能会将其直接编码为一个token。



# 1.2 回答问题

## 1.2.1 Python中使用什么函数查看字符的Unicode，什么函数将Unicode转换成字符？并使用它们查看“北”“大”的Unicode，查看Unicode为22823、27169、22411对应的字符。

- **查看字符的Unicode：**
  在Python中，可以使用`ord()`函数查看字符的Unicode编码。`ord()`函数返回一个字符对应的Unicode码点。

  ```python
  print(ord('北'))  # 输出: 22823
  print(ord('大'))  # 输出: 27169
  ```

- **将Unicode转换成字符：**
  要将Unicode码点转换回字符，可以使用`chr()`函数。`chr()`函数接受一个Unicode码点，返回对应的字符。

  ```python
  print(chr(22823))  # 输出: 北
  print(chr(27169))  # 输出: 大
  print(chr(22411))  # 输出: 查（对应Unicode码为22411）
  ```

## 1.2.2 Tokenizer的vocab size大和小分别有什么好处和坏处？

- vocab size大：
  - 好处：
    - 能够表示更多的单词、词组或语法单元，因此对各种文本有更高的覆盖率。
    - 能够更准确地表示常见的单词和短语，不容易拆分成多个子词。
  - 坏处：
    - 需要更多的存储空间，词汇表更大，内存占用更高。
    - 词汇表过大时，稀疏性增大，可能出现词频分布不均，导致部分词汇频次较低的情况下学习效果不好。
    - 训练时间长，模型的推理速度可能变慢。

- vocab size小：
  - 好处：
    - 词汇表小，内存占用低，模型训练和推理速度更快。
    - 对低频词进行拆分，可以让模型学习更多的子词表示，提升泛化能力。
  - 坏处：
    - 可能无法完整表示一些常见的词，导致需要频繁地将词拆解为多个子词，影响模型性能。
    - 对于长尾词的处理不如大vocab size的tokenizer有效，导致模型的词汇覆盖率较低。

## 1.2.3 为什么 LLM 不能处理非常简单的字符串操作任务，比如反转字符串？

LLM主要是通过概率和上下文推测生成文本，而不是通过明确的编程逻辑来执行任务。反转字符串等简单的字符串操作通常需要明确的步骤和控制流逻辑，但LLM并不具备这种明确的逻辑推理能力。它处理的是基于语言的概率分布，并且对于类似编程任务，往往依赖上下文学习，但很难推理出正确的操作步骤。

## 1.2.4 为什么 LLM 在非英语语言（例如日语）上表现较差？

LLM在非英语语言上表现较差，通常是因为训练数据中英语占主导地位，非英语语言的训练数据较少。因此，模型对非英语语言的语法、词汇、句子结构等理解能力较弱。尤其是一些语言（如日语）具有复杂的书写系统和语法规则，且词汇表可能需要更多的子词切分，模型需要更多的非英语数据来进行有效训练。

## 1.2.5 为什么 LLM 在简单算术问题上表现不好？

LLM通常通过预测下一个词的概率来生成文本，而不是像传统的计算机程序那样执行具体的数值运算。对于简单的算术问题，LLM通常是基于语言模式来生成答案，而不是通过执行数学计算。因此，模型在算术问题上容易出错，特别是当问题涉及多步骤运算时，LLM更容易偏离正确答案。

## 1.2.6 为什么 GPT-2 在编写 Python 代码时遇到比预期更多的困难？

GPT-2是一个语言模型，主要通过预测文本来生成答案。虽然它可以生成合理的自然语言文本，但编写Python代码需要精确的语法、逻辑和结构。GPT-2没有显式的编程语法学习机制，也没有进行特定编程任务的优化训练。因此，它在编写代码时可能会犯一些语法错误，或者没有足够的上下文来推断出正确的逻辑，导致更多的困难。

## 1.2.7 为什么 LLM 遇到字符串 “<|endoftext|>” 时会突然中断？

`"<|endoftext|>"` 是GPT类模型等语言模型在训练时用来表示文本的结束标记。在训练过程中，模型被训练去预测文本直到遇到这个特殊标记，因此，当模型生成文本时遇到这个标记，表示生成任务的结束，模型会停止输出。

## 1.2.8 为什么当问 LLM 关于 “SolidGoldMagikarp” 的问题时 LLM 会崩溃？

"SolidGoldMagikarp" 是一种网络迷因（meme）或特定语境中的词汇，可能是某个特定领域的术语或者与特定知识库相关。LLM在遇到这种非常特殊的输入时，若没有相关训练数据或上下文，它可能无法正确处理这个词汇，从而出现崩溃或者生成不正确的答案。

## 1.2.9 为什么在使用 LLM 时应该更倾向于使用 YAML 而不是 JSON？

YAML相比JSON具有更简洁的语法，尤其是在处理复杂的嵌套结构时更加直观易读。LLM处理文本时可以更容易理解YAML格式的语义，并能够生成结构化的输出。JSON语法虽然更严格，但可能更难与自然语言交互。YAML对于人类编辑更加友好，因此在与LLM交互时，YAML更容易生成符合预期的结果。

## 1.2.10 为什么 LLM 实际上不是端到端的语言建模？

LLM虽然是在预训练阶段通过大量的文本数据进行训练的，但它的生成过程并不是完全端到端的语言建模。因为LLM的目标是预测下一个词或标记，并生成连贯的文本，而不是执行某种特定的任务或者逻辑推理。端到端的语言建模通常意味着模型能够在一个输入到输出的过程中，经过少量的手动干预或规则，直接得到正确的输出。而LLM往往需要较长的上下文信息，且它的输出依赖于训练数据中的概率分布，而不是明确的规则推理。
